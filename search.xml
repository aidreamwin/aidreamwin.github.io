<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[学习笔记（六）TensorFlow可视化TensorBoard]]></title>
    <url>%2F2017%2F12%2F30%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89tensorflow%E5%8F%AF%E8%A7%86%E5%8C%96tensorboard%2F</url>
    <content type="text"><![CDATA[前言在做深度学习有关学习研究中，少不了使用可视化工具，TensorFlow中提供了自带的可视化工具。这样训练模型时候可以看到模型学习的实时状况，对于调整参数来说，确实帮助很大。下面动手简单的实现TensorBoard可视化功能。 正文步骤要使用TensorFlow可视化工具TensorBoard首先需要在代码中设置需要观察的点。 一般像权值这样的向量用histogram来记录。如示例代码1中tf.summary.histogram(‘w1’, w1)就是记录权值w1；第一个参数为该记录的名字name，第二个参数为记录的值。 示例代码1 12345with tf.name_scope(&quot;Weight&quot;): w1 = tf.Variable(tf.random_normal([2,3],stddev=1),name=&quot;w1&quot;) w2 = tf.Variable(tf.random_normal([3,1],stddev=1),name=&quot;w2&quot;) tf.summary.histogram(&apos;w1&apos;, w1) tf.summary.histogram(&apos;w2&apos;, w2) 对于loss，一般用scalar记录。如下代码所示，和histogram用法一样。 示例代码2 1tf.summary.scalar(&apos;cross_entropy&apos;, cross_entropy) 因为之前定义了多个summary，逐一执行太过麻烦，所以这里使用tf.summary.merge_all()直接获取所有汇总操作。 示例代码3 1merged = tf.summary.merge_all() 然后定义文件记录器，存放训练的日志数据。将Session的计算图加入到训练过程的记录器，这样在TensorBoard的GRAPHS窗口中就能展示整个计算图的可视化效果。 示例代码4 1writer = tf.summary.FileWriter(&quot;/home/dylan/study/jupyter/log&quot;, sess.graph) 最后一步，计算在当前状态下所有的summary，并将结果写入日志数据中。 示例代码5 12summary,total_cross_entropy = sess.run([merged,cross_entropy],feed_dict=&#123;x:X,y_:Y&#125;)writer.add_summary(summary, i) 完整示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#coding=utf-8# 神经网络import tensorflow as tffrom numpy.random import RandomStatetf.reset_default_graph()#定义训练数据batch大小batch_size = 8#权值with tf.name_scope(&quot;Weight&quot;): w1 = tf.Variable(tf.random_normal([2,3],stddev=1),name=&quot;w1&quot;) w2 = tf.Variable(tf.random_normal([3,1],stddev=1),name=&quot;w2&quot;) tf.summary.histogram(&apos;w1&apos;, w1) tf.summary.histogram(&apos;w2&apos;, w2)with tf.name_scope(&quot;Biases&quot;): b1 = tf.Variable(tf.zeros([1, 3]) + 0.1, name=&apos;b1&apos;) b2 = tf.Variable(tf.zeros([1, 1]) + 0.1, name=&apos;b2&apos;)#定义输入占位符和反向传播占位符with tf.name_scope(&quot;Input&quot;): x = tf.placeholder(tf.float32, shape=(None,2), name=&quot;x-input&quot;) y_ = tf.placeholder(tf.float32, shape=(None,1), name=&quot;y-input&quot;)#定义前向传播过程with tf.name_scope(&quot;Layer&quot;): a = tf.add(tf.matmul(x,w1),b1)with tf.name_scope(&quot;Output&quot;): y = tf.nn.relu(tf.add(tf.matmul(a,w2),b2))#定义损失函数with tf.name_scope(&quot;loss&quot;): cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0))) tf.summary.scalar(&apos;cross_entropy&apos;, cross_entropy)with tf.name_scope(&quot;train&quot;): train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)#通过随机数产生模拟数据集rdm = RandomState(1)dataset_size = 128X = rdm.rand(dataset_size,2)Y = [[int(x1+x2&lt;1)] for (x1,x2) in X]#print(X)#print(Y)#创建一个会话来运行with tf.Session() as sess: merged = tf.summary.merge_all() writer = tf.summary.FileWriter(&quot;/home/dylan/study/jupyter/log&quot;, sess.graph) tf.global_variables_initializer().run() #print(sess.run(w1)) #print(sess.run(w2)) STEPS = 5000 for i in range(STEPS): start = (i * batch_size) % dataset_size end = min(start+batch_size,dataset_size) #使用样本训练并更新参数 sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;) if i % 200 == 0: #计算一段时间内的交叉熵并输出 summary,total_cross_entropy = sess.run([merged,cross_entropy],feed_dict=&#123;x:X,y_:Y&#125;) print(&quot;After %d training step(s),cross_entropy on all data is %g&quot; % (i,total_cross_entropy)) writer.add_summary(summary, i) #print(sess.run(w1)) #print(sess.run(w2)) #writer = tf.summary.FileWriter(&quot;./log&quot;,tf.get_default_graph()) writer.close() 输出 运行结果图（神经网络结构图） 运行结果图（交叉熵），可以看到交叉熵在不断减小，说明神经网络学到东西了。 运行结果图 （权值w1,w2）]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（五）tensorflow卷积神经网络MNIST手写数字识别]]></title>
    <url>%2F2017%2F12%2F18%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89tensorflow%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CMNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[卷积神经网络MNIST手写数字识别MNIST手写数学识别就像C语言的“hello world”一样，是学习人工智能和图片识别经典必修例子。 MNIST数据库介绍：MNIST是一个手写数字数据库，它有60000个训练样本集和10000个测试样本集。它是NIST数据库的一个子集。 MNIST数据库可以去官网下载，也可以直接在代码中自动下载，train-images-idx3-ubyte.gz、train-labels-idx1-ubyte.gz等。下载四个文件，解压缩。解压缩后发现这些文件并不是标准的图像格式。这些图像数据都保存在二进制文件中。每个样本图像的宽高为28*28。 过程简述导入MNIST数据集导入MNIST数据集可以使用tensorflow封装的input_data，使用非常方便。如果数据集存在就直接使用，不存在就会自动下载。在实际中可能会由于墙外的资源访问比较慢，建议先去官网下载。 自动下载和安装MNIST数据集，该数据会保存在当前目录MNIST_data下。 123import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) 创建一个交互式Session1sess = tf.InteractiveSession() 创建两个占位符shape是占位符分维度，第一个参数设置的None，它会自动计算维度。 x是输入特征，y_是期望 12x = tf.placeholder(&quot;float&quot;, shape=[None, 784])y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10]) 初始化权重及偏置1234567891011#权重初始化函数def weight_variable(shape): #输出服从截尾正态分布的随机值 initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#偏置初始化函数def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 创建卷积x 是一个4维张量，shape为[batch,height,width,channels]，卷积核移动步长为1。填充类型为SAME,可以不丢弃任何像素点 123def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=&quot;SAME&quot;) 创建池化采用最大池化，也就是取窗口中的最大值作为结果。x 是一个4维张量，shape为[batch,height,width,channels]；ksize表示pool窗口大小为2x2,也就是高2，宽2；strides，表示在height和width维度上的步长都为2； 123def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&quot;SAME&quot;) 前向传播和激活函数ReLU把x_image和权重进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling。h_pool1的输出即为第一层网络输出，shape为[batch,14,14,1] 1h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) 使用全连接层这层是拥有1024个神经元的全连接层，W的第1维size为7764，7*7是h_pool2输出的size，64是第2层输出神经元个数。 12W_fc1 = weight_variable([7*7*64, 1024])b_fc1 = bias_variable([1024]) Dropout层为了减少过拟合，在输出层前加入dropout 12keep_prob = tf.placeholder(&quot;float&quot;)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) softmax层输出时使用softmax将网络输出值转换成了概率 1y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 交叉墒预测值和真实值之间的交叉墒 1cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv)) 梯度下降使用ADAM优化器来做梯度下降。学习率为0.0001 1train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) 评估模型tf.argmax能给出某个tensor对象在某一维上数据最大值的索引。因为标签是由0,1组成了one-hot vector，返回的索引就是数值为1的位置 1correct_predict = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1)) 精确度计算计算正确预测项的比例，因为tf.equal返回的是布尔值，使用tf.cast把布尔值转换成浮点数，然后用tf.reduce_mean求平均值 1accuracy = tf.reduce_mean(tf.cast(correct_predict, &quot;float&quot;)) 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# -*- coding: utf-8 -*- import tensorflow as tf#导入input_data用于自动下载和安装MNIST数据集from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)#创建一个交互式Sessionsess = tf.InteractiveSession()#创建两个占位符，x为输入网络的图像，y_为输入网络的图像类别x = tf.placeholder(&quot;float&quot;, shape=[None, 784])y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10])#权重初始化函数def weight_variable(shape): #输出服从截尾正态分布的随机值 initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#偏置初始化函数def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)#创建卷积op#x 是一个4维张量，shape为[batch,height,width,channels]#卷积核移动步长为1。填充类型为SAME,可以不丢弃任何像素点def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=&quot;SAME&quot;)#创建池化op#采用最大池化，也就是取窗口中的最大值作为结果#x 是一个4维张量，shape为[batch,height,width,channels]#ksize表示pool窗口大小为2x2,也就是高2，宽2#strides，表示在height和width维度上的步长都为2def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&quot;SAME&quot;)#第1层，卷积层#初始化W为[5,5,1,32]的张量，表示卷积核大小为5*5，第一层网络的输入和输出神经元个数分别为1和32##########################11111111111111W_conv1 = weight_variable([5,5,1,32])#初始化b为[32],即输出大小b_conv1 = bias_variable([32])#把输入x(二维张量,shape为[batch, 784])变成4d的x_image，x_image的shape应该是[batch,28,28,1]#-1表示自动推测这个维度的sizex_image = tf.reshape(x, [-1,28,28,1])#把x_image和权重进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling#h_pool1的输出即为第一层网络输出，shape为[batch,14,14,1]h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)##########################22222222222h_pool1 = max_pool_2x2(h_conv1)#第2层，卷积层#卷积核大小依然是5*5，这层的输入和输出神经元个数为32和64##########################3333333W_conv2 = weight_variable([5,5,32,64])b_conv2 = weight_variable([64])#h_pool2即为第二层网络输出，shape为[batch,7,7,1]h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)##########################4444444h_pool2 = max_pool_2x2(h_conv2)#第3层, 全连接层#这层是拥有1024个神经元的全连接层#W的第1维size为7*7*64，7*7是h_pool2输出的size，64是第2层输出神经元个数##########################5555555W_fc1 = weight_variable([7*7*64, 1024])b_fc1 = bias_variable([1024])#计算前需要把第2层的输出reshape成[batch, 7*7*64]的张量h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)#Dropout层#为了减少过拟合，在输出层前加入dropoutkeep_prob = tf.placeholder(&quot;float&quot;)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)#输出层#最后，添加一个softmax层#可以理解为另一个全连接层，只不过输出时使用softmax将网络输出值转换成了概率W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)#预测值和真实值之间的交叉墒cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))#train op, 使用ADAM优化器来做梯度下降。学习率为0.0001train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#评估模型，tf.argmax能给出某个tensor对象在某一维上数据最大值的索引。#因为标签是由0,1组成了one-hot vector，返回的索引就是数值为1的位置correct_predict = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))#计算正确预测项的比例，因为tf.equal返回的是布尔值，#使用tf.cast把布尔值转换成浮点数，然后用tf.reduce_mean求平均值accuracy = tf.reduce_mean(tf.cast(correct_predict, &quot;float&quot;))#初始化变量sess.run(tf.global_variables_initializer())#开始训练模型，循环20000次，每次随机从训练集中抓取50幅图像for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: #每100次输出一次日志 train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_:batch[1], keep_prob:1.0&#125;) print (&quot;step %d, training accuracy %g&quot; % (i, train_accuracy)) train_step.run(feed_dict=&#123;x:batch[0], y_:batch[1], keep_prob:0.5&#125;)print (&quot;test accuracy %g&quot; % accuracy.eval(feed_dict=&#123;x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0&#125;)) 运行结果 训练所得结果如下图]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（三）tensorflow完整神经网络算法]]></title>
    <url>%2F2017%2F12%2F10%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89tensorflow%E5%AE%8C%E6%95%B4%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[完整神经网络经过前面的学习，对一个完整神经网路的实践已经迫不及待，下面的代码将展示关于神经网络的完整结构 定义输入占位符和反向传播占位符 初始化权值及偏置 定义前向传播过程 定义损失函数交叉熵 创建一个会话来运行 使用样本训练并更新参数 得到最终结果权值 要使用tensorflow首先需要导入tensorflow模块 1import tensorflow as tf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#coding=utf-8# 神经网络import tensorflow as tffrom numpy.random import RandomState#定义训练数据batch大小batch_size = 8#权值w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1),name="w1")w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1),name="w2")#定义输入占位符和反向传播占位符x = tf.placeholder(tf.float32, shape=(None,2), name="x-input")y_ = tf.placeholder(tf.float32, shape=(None,1), name="y-input")#定义前向传播过程a = tf.matmul(x,w1)y = tf.matmul(a,w2)#定义损失函数cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)#通过随机数产生模拟数据集rdm = RandomState(1)dataset_size = 128X = rdm.rand(dataset_size,2)Y = [[int(x1+x2&lt;1)] for (x1,x2) in X]#创建一个会话来运行with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) print(sess.run(w1)) print(sess.run(w2)) STEPS = 5000 for i in range(STEPS): start = (i * batch_size) % dataset_size end = min(start+batch_size,dataset_size) #使用样本训练并更新参数 sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;) if i % 1000 == 0: #计算一段时间内的交叉熵并输出 total_cross_entropy = sess.run(cross_entropy,feed_dict=&#123;x:X,y_:Y&#125;) print("After %d training step(s),cross_entropy on all data is %g" % (i,total_cross_entropy)) print(sess.run(w1)) print(sess.run(w2)) writer = tf.summary.FileWriter("./log",tf.get_default_graph()) writer.close() 输出结果: [[-0.81131822 1.48459876 0.06532937] [-2.4427042 0.0992484 0.59122431]] [[-0.81131822] [ 1.48459876] [ 0.06532937]] After 0 training step(s),cross_entropy on all data is 0.0674925 After 1000 training step(s),cross_entropy on all data is 0.0163385 After 2000 training step(s),cross_entropy on all data is 0.00907547 After 3000 training step(s),cross_entropy on all data is 0.00714436 After 4000 training step(s),cross_entropy on all data is 0.00578471 [[-1.96182752 2.58235407 1.68203771] [-3.46817183 1.06982315 2.11788988]] [[-1.82471502] [ 2.68546653] [ 1.41819501]] 通过tensorBoard生成结构图]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（二）tensorflow神经网络前向传播算法]]></title>
    <url>%2F2017%2F12%2F09%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[神经网络程序员学习的时候最重要的就是实践，以下代码是我自己手写，在自己搭建的jupyter tensorflow交互式环境下运行生成的。 要使用tensorflow首先需要导入tensorflow模块 1import tensorflow as tf constant123456789a = tf.constant([1.0,2.0],name="a",dtype=tf.float64)b = tf.constant([2.0,3.0],name="b",dtype=tf.float64)result = a + bprint(result)sess = tf.Session()x = sess.run(result)print(result)print(x)sess.close() 输出 Tensor(&quot;add_6:0&quot;, shape=(2,), dtype=float64) Tensor(&quot;add_6:0&quot;, shape=(2,), dtype=float64) [ 3. 5.] add_3:0表示result这个张量是计算节点“add”输出的第一个结果（编号从0开始） 会话 tensorflow的运算必须在会话（session）中计算 12345678910111213a = tf.constant([1.0,2.0],name="a",dtype=tf.float64)b = tf.constant([2.0,3.0],name="b",dtype=tf.float64)result = a + bprint(result)#交互式会话，使用这个函数会自动的将生成的会话注册为默认会话sess = tf.InteractiveSession()#sess = tf.Session()try: print(result.eval())except Exception as e: print(e) sess.close()sess.close() 输出 Tensor(&quot;add_10:0&quot;, shape=(2,), dtype=float64) [ 3. 5.] Variable123#随机生成矩阵，正态分布，标准差stddev=2weights = tf.Variable(tf.random_normal([2,3],stddev=2,dtype=tf.float64),name="weights")print(weights) 输出 &lt;tf.Variable &apos;weights_8:0&apos; shape=(2, 3) dtype=float64_ref&gt; 前向传播算法 神经网络的前向传播算法，简单的说就是矩阵的传播算法 123456789101112131415161718192021import tensorflow as tf#声明w1,w2两个变量，seed参数设定随机种子，这样保证每次运行的得到的结果一样w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1),name="w1")w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1),name="w2")# 假设输入的特征向量为一个常量，1*2的矩阵x = tf.constant([[0.7,0.9]],name="x")#前向传播算法a = tf.matmul(x,w1)y = tf.matmul(a,w2)#初始化变量with tf.Session() as sess: #writer = tf.train.SummaryWriter("./log",tf.get_default_graph()) writer = tf.summary.FileWriter("./log",tf.get_default_graph()) sess.run(tf.global_variables_initializer()) print(y.eval()) writer.close() 输出 [[ 3.95757794]] placeholder和前向传播算法 通过使用placeholder 实现神经网络的前向传播，placeholder是tensorflow中的占位符 12345678910111213141516import tensorflow as tfw1 = tf.Variable(tf.random_normal([2,3],stddev=1),name="w1")w2 = tf.Variable(tf.random_normal([3,1],stddev=1),name="w2")x = tf.placeholder(tf.float32,shape=(3,2),name="input")a = tf.matmul(x,w1)y = tf.matmul(a,w2)with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) result = sess.run(y,feed_dict=&#123;x:[[0.7,0.9],[0.1,0.4],[0.5,0.8]]&#125;) writer = tf.summary.FileWriter("./log",tf.get_default_graph()) writer.close() print(result) 输出 [[-4.83675146] [-1.65250468] [-4.0115037 ]] 损失函数、交叉熵12345678#定义损失函数刻画与测试和真实值的差距,交叉熵cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))#定义学习率learning_rate = 0.001#定义反向传播算法优化参数，AdamOptimizer优化算法的一种，目前有7种train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（一）tensorflow+jupyter交互式环境搭建]]></title>
    <url>%2F2017%2F11%2F20%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89tensorflow%2Bjupyter%E4%BA%A4%E4%BA%92%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[tensorflow环境搭建经过对吴恩达机器学习课程的学习，理解了有关机器学习的基本理论，于是向着兴趣深度学习（deep learning）迈进。 基本步骤 安装Ubuntu 安装Python 安装Pip 安装Tensorflow 安装Jupyter 安装Nginx 绑定域名 使用交互式环境开发 安装Ubuntu64位Ubuntu作为人工智能学习最搭配的操作系统成为首选，关于Ubuntu安装这里不做详细阐述。 安装Python对tensorflow支持最好的python版本是3.5 ，所以这里使用python3。在Ubuntu系统终端中输入下面的命令安装： 1$ sudo apt-get install python3 安装Pippip是python的包管理器，使用pip安装python的各种模块非常方便，安装命令： 注意这里使用的是python3，因此pip版本也要安装pip3 1$ sudo apt-get install python3-pip 安装TensorflowTensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统，其命名来源于本身的运行原理。Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。 TensorFlow可被用于语音识别或图像识别等多项机器深度学习领域，对2011年开发的深度学习基础架构DistBelief进行了各方面的改进，它可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行。TensorFlow将完全开源，任何人都可以用。 有了上面的python3和pip3的安装，安装tensorflow就非常简单了，安装命令： 安装Tensorflow 1$ sudo pip3 install tensorflow 安装numpy numpy是一个用python实现的科学计算包。包括： 一个强大的N维数组对象Array； 比较成熟的（广播）函数库； 用于整合C/C++和Fortran代码的工具包； 实用的线性代数、傅里叶变换和随机数生成函数。 numpy和稀疏矩阵运算包scipy配合使用更加方便。 1$ sudo pip3 install numpy 安装SciPy SciPy是一款方便、易于使用、专为科学和工程设计的Python工具包.它包括统计,优化,整合,线性代数模块,傅里叶变换,信号和图像处理,常微分方程求解器等等. 1$ sudo pip3 install scipy 安装Pandas Python Data Analysis Library 或 pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。 1$ sudo pip3 install pandas 安装Matplotlib Matplotlib 是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。 1$ sudo pip3 install matplotlib 安装JupyterJupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。 Jupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。 数据挖掘领域中最热门的比赛 Kaggle 里的资料都是Jupyter格式。 1$ sudo pip3 install jupyter 启动命令 1$ jupyter notebook 然后在浏览器中输入刚刚启动界面显示的地址，比如我的地址是下面的： 1http://localhost:8888/?token=04cb8b5512b8680806499babab90c165dbca882adbe7b484 进入交互界面 尽情的使用吧 下面将讲述的是搭建基于jupyter的远程网络web服务器，这样不管走在哪里，使用手机还是电脑终端都可以使用自己的jupyter交互式环境开发学习。如果不需要搭建远程服务器，下面的可以略过 进行下面的步骤之前确认： 上面的环境都是在远程服务器上安装的（比如阿里云服务器） 购买域名（如果没有，使用ip地址访问） 安装NginxNginx (engine x) 是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。 1$ sudo apt-get install nginx 配置NGINX服务器 1$ sudo vi /etc/nginx/nginx.conf 在第13行# Basic Settings后面加上自己的配置 123456789101112131415161718192021222324252627282930313233upstream jupyter &#123; #jupyter所对应的ip和端口 ip_hash; #会话保存 server ip:端口; #如 192.23.12.12:8761 &#125; upstream tb &#123; #tensorflow可视化工具配置所对应的ip和端口 ip_hash; #session_sticky; # cookie=uid fallback=on mode=insert option=indirect;这是另一种配置，安装比较麻烦，这里不讲 server ip:端口; #如 192.23.12.12:8761 &#125; server &#123; server_name jupyter.域名; #如 jupyter.baidu.com #ssl证书文件位置(常见证书文件格式为：crt/pem) ssl_certificate /home/key/mycert.pem; #ssl证书key位置 ssl_certificate_key /home/key/mykey.key; #root /home/web/rootadmin_blog; location / &#123; proxy_pass http://jupyter; proxy_connect_timeout 600; proxy_read_timeout 600; #下面的不用改 但必须配上 不然交互式界面没有会话保存 ，不会跳转 proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # WebSocket support proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; &#125; &#125; 绑定域名设置域名解析，设置两条解析记录 1234# 这条记录让你的加不加www都一样（此处可以不配）Type:AHost:*IP:你的服务器ip 1234#这条很关键，可以让你在NGINX上任意映射端口和二级域名Type:AHost:@IP:你的服务器ip 使用交互式环境开发如果你的服务器上安装有防火墙ufw，需要打开80端口 1sudo ufw allow 80 在浏览器中使用自己的域名访问：jupyter.域名（如 jupyter.baidu.com）]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[品三国]]></title>
    <url>%2F2012%2F08%2F13%2F%E5%93%81%E4%B8%89%E5%9B%BD%2F</url>
    <content type="text"><![CDATA[三品三国： 一品三国，觉张飞之猛，关羽之傲，赵云之勇，天下无敌！ 二品三国，察孔明之智，公瑾之谋，仲达之奸，无之匹敌！ 再品三国，然国之奸雄，能谋善断者，独有孟德！三战徐州，官渡败袁，远征乌桓，平定关中，破黄巾，斩吕布，一生豪迈，诗情画意，短歌行，观沧海。临危制变，料敌设奇，赤壁虽败，难掩奇才！ 初偿三国，尢爱子龙，百万曹军，来去自如，长坂救嫂，七进七出，探囊取物，忠勇无敌。 次之，张飞关羽。力斩邓茂，鞭挞督邮，虎牢大战，大破徐州，当阳疑兵，吓退曹骑，计败张郃，此张飞也！ 阵斩颜良，单刀赴宴，温酒斩华雄，千里走单骑，此关羽也！ 后喜，诸葛孔明，躬耕南阳，隆中预言，天下三分，稳中求胜，百战不殆，草船借箭，空城御敌，出师表赋，更彰文采！ 公瑾周瑜，水师五万，赤壁败曹！ 司马仲达，大智若愚，韬光养晦，不鸣则以，一鸣惊人，斩孟达，取街亭，阻北伐！ 然，乱世三国，群雄角逐，各路英雄，皆亚曹操，东征西讨，用兵如神，礼贤下士，纵横天下，成王败寇，亘古不变！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
</search>
