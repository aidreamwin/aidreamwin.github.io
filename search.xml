<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python logging 的简单封装]]></title>
    <url>%2F2018%2F05%2F26%2FPython%20logging%20%E7%9A%84%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Python logging前言平时在Python程序中记录程序运行情况或者打印各种重要数据总是用“print”语句很多时候不方便，因为它只能把日志打印在屏幕上，并且不能分级显示。于是乎我对Python社区提供的一个功能强大的标准日志模块进行了封装，以便更加方便的使用。 正文废话不多说，直接上代码 12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding: utf-8 -*-import logging.handlersimport logging,os class Logger: def __init__(self, path = './test.log',clevel = logging.DEBUG,Flevel = logging.DEBUG): self.logger = logging.getLogger(path) self.logger.setLevel(logging.DEBUG) fmt = logging.Formatter('%(asctime)s|%(levelname)s|%(message)s') sh = logging.StreamHandler() sh.setFormatter(fmt) sh.setLevel(clevel) fh = logging.handlers.RotatingFileHandler(path, maxBytes=10*1024, backupCount=50) fh.encoding="utf-8" fh.setFormatter(fmt) fh.setLevel(Flevel) self.logger.addHandler(sh) self.logger.addHandler(fh) def debug(self,message): self.logger.debug(message) def info(self,message): self.logger.info(message) def war(self,message): self.logger.warn(message) def error(self,message): self.logger.error(message) def cri(self,message): self.logger.critical(message) def test(): log_test = Logger('./test.log',logging.DEBUG,logging.DEBUG) log_test.debug('test log')if __name__ =='__main__': test() 如果直接在log中运行程序，结果如下： 123$ python ./log.py# 2018-05-26 21:58:53,304|DEBUG|test log 这个时候说明程序没有问题，可以当作模块使用了，于是乎你可以在其他模块中调用它了。 1234567891011121314# -*- coding: utf-8 -*-import syssys.path.append("../..")from BinLog.log import Logger# Logger('./test.log',logging.DEBUG,logging.DEBUG)x = Logger()x.debug("hello1")x.debug("hello2")x.debug("hello3")x.debug("hello4")x.debug("hello5")x.debug("hello6") END]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>组件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（七）tensorflow保存读取]]></title>
    <url>%2F2017%2F12%2F30%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89tensorflow%E4%BF%9D%E5%AD%98%E8%AF%BB%E5%8F%96%2F</url>
    <content type="text"><![CDATA[前言在使用TensorFlow中训练好一个模型之后，往往需要将权值，偏置等数据保存，以便在今后再跑模型时候不需要再次训练。 TensorFlow目前提供了一套保存以及恢复模型数据的接口方法。 简单说明保存读取保存为了简便操作以及理解，定义申明两个变量，权值和偏置，然后把数据保存起来。 1234567891011121314import tensorflow as tfimport numpy as npW = tf.Variable([[3,2,1],[9,8,5]], dtype=tf.float32, name='weights')B = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases')init=tf.global_variables_initializer()##saver = tf.train.Saver()with tf.Session() as sess: sess.run(init) save_path = saver.save(sess, "config/save.ckpt") print("Save to path: ", save_path) 输出 Save to path: config/save.ckpt 保存目录下的文件 恢复恢复的时候定义的数据的结构必须和保存的时候的结构一致，不仅需要一致，而且数据类型也必须相同。 比如，刚刚上边用的dtype=tf.float32，这里恢复的时候也必须使用dtype=tf.float32，name也需要和保存的时候一样。 1234import tensorflow as tfimport numpy as npW2 = tf.Variable(np.arange(6).reshape((2, 3)), dtype=tf.float32, name="weights")B2 = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name="biases") 这里我们不需要再对数据进行初始化了，我们直接加载保存的数据 12345saver = tf.train.Saver()with tf.Session() as sess: saver.restore(sess, "config/save.ckpt") print("weights:", sess.run(W2)) print("biases:", sess.run(B2)) 输出 INFO:tensorflow:Restoring parameters from config/save.ckpt weights: [[ 3. 2. 1.] [ 9. 8. 5.]] biases: [[ 1. 2. 3.]]]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（六）TensorFlow可视化TensorBoard]]></title>
    <url>%2F2017%2F12%2F30%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89tensorflow%E5%8F%AF%E8%A7%86%E5%8C%96tensorboard%2F</url>
    <content type="text"><![CDATA[前言在做深度学习有关学习研究中，少不了使用可视化工具，TensorFlow中提供了自带的可视化工具。这样训练模型时候可以看到模型学习的实时状况，对于调整参数来说，确实帮助很大。下面动手简单的实现TensorBoard可视化功能。 正文步骤要使用TensorFlow可视化工具TensorBoard首先需要在代码中设置需要观察的点。 一般像权值这样的向量用histogram来记录。如示例代码1中tf.summary.histogram(‘w1’, w1)就是记录权值w1；第一个参数为该记录的名字name，第二个参数为记录的值。 示例代码1 12345with tf.name_scope(&quot;Weight&quot;): w1 = tf.Variable(tf.random_normal([2,3],stddev=1),name=&quot;w1&quot;) w2 = tf.Variable(tf.random_normal([3,1],stddev=1),name=&quot;w2&quot;) tf.summary.histogram(&apos;w1&apos;, w1) tf.summary.histogram(&apos;w2&apos;, w2) 对于loss，一般用scalar记录。如下代码所示，和histogram用法一样。 示例代码2 1tf.summary.scalar(&apos;cross_entropy&apos;, cross_entropy) 因为之前定义了多个summary，逐一执行太过麻烦，所以这里使用tf.summary.merge_all()直接获取所有汇总操作。 示例代码3 1merged = tf.summary.merge_all() 然后定义文件记录器，存放训练的日志数据。将Session的计算图加入到训练过程的记录器，这样在TensorBoard的GRAPHS窗口中就能展示整个计算图的可视化效果。 示例代码4 1writer = tf.summary.FileWriter(&quot;/home/dylan/study/jupyter/log&quot;, sess.graph) 最后一步，计算在当前状态下所有的summary，并将结果写入日志数据中。 示例代码5 12summary,total_cross_entropy = sess.run([merged,cross_entropy],feed_dict=&#123;x:X,y_:Y&#125;)writer.add_summary(summary, i) 完整示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#coding=utf-8# 神经网络import tensorflow as tffrom numpy.random import RandomStatetf.reset_default_graph()#定义训练数据batch大小batch_size = 8#权值with tf.name_scope(&quot;Weight&quot;): w1 = tf.Variable(tf.random_normal([2,3],stddev=1),name=&quot;w1&quot;) w2 = tf.Variable(tf.random_normal([3,1],stddev=1),name=&quot;w2&quot;) tf.summary.histogram(&apos;w1&apos;, w1) tf.summary.histogram(&apos;w2&apos;, w2)with tf.name_scope(&quot;Biases&quot;): b1 = tf.Variable(tf.zeros([1, 3]) + 0.1, name=&apos;b1&apos;) b2 = tf.Variable(tf.zeros([1, 1]) + 0.1, name=&apos;b2&apos;)#定义输入占位符和反向传播占位符with tf.name_scope(&quot;Input&quot;): x = tf.placeholder(tf.float32, shape=(None,2), name=&quot;x-input&quot;) y_ = tf.placeholder(tf.float32, shape=(None,1), name=&quot;y-input&quot;)#定义前向传播过程with tf.name_scope(&quot;Layer&quot;): a = tf.add(tf.matmul(x,w1),b1)with tf.name_scope(&quot;Output&quot;): y = tf.nn.relu(tf.add(tf.matmul(a,w2),b2))#定义损失函数with tf.name_scope(&quot;loss&quot;): cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0))) tf.summary.scalar(&apos;cross_entropy&apos;, cross_entropy)with tf.name_scope(&quot;train&quot;): train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)#通过随机数产生模拟数据集rdm = RandomState(1)dataset_size = 128X = rdm.rand(dataset_size,2)Y = [[int(x1+x2&lt;1)] for (x1,x2) in X]#print(X)#print(Y)#创建一个会话来运行with tf.Session() as sess: merged = tf.summary.merge_all() writer = tf.summary.FileWriter(&quot;/home/dylan/study/jupyter/log&quot;, sess.graph) tf.global_variables_initializer().run() #print(sess.run(w1)) #print(sess.run(w2)) STEPS = 5000 for i in range(STEPS): start = (i * batch_size) % dataset_size end = min(start+batch_size,dataset_size) #使用样本训练并更新参数 sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;) if i % 200 == 0: #计算一段时间内的交叉熵并输出 summary,total_cross_entropy = sess.run([merged,cross_entropy],feed_dict=&#123;x:X,y_:Y&#125;) print(&quot;After %d training step(s),cross_entropy on all data is %g&quot; % (i,total_cross_entropy)) writer.add_summary(summary, i) #print(sess.run(w1)) #print(sess.run(w2)) #writer = tf.summary.FileWriter(&quot;./log&quot;,tf.get_default_graph()) writer.close() 输出 运行结果图（神经网络结构图） 运行结果图（交叉熵），可以看到交叉熵在不断减小，说明神经网络学到东西了。 运行结果图 （权值w1,w2）]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（五）tensorflow卷积神经网络MNIST手写数字识别]]></title>
    <url>%2F2017%2F12%2F18%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89tensorflow%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CMNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[卷积神经网络MNIST手写数字识别MNIST手写数学识别就像C语言的“hello world”一样，是学习人工智能和图片识别经典必修例子。 MNIST数据库介绍：MNIST是一个手写数字数据库，它有60000个训练样本集和10000个测试样本集。它是NIST数据库的一个子集。 MNIST数据库可以去官网下载，也可以直接在代码中自动下载，train-images-idx3-ubyte.gz、train-labels-idx1-ubyte.gz等。下载四个文件，解压缩。解压缩后发现这些文件并不是标准的图像格式。这些图像数据都保存在二进制文件中。每个样本图像的宽高为28*28。 过程简述导入MNIST数据集导入MNIST数据集可以使用tensorflow封装的input_data，使用非常方便。如果数据集存在就直接使用，不存在就会自动下载。在实际中可能会由于墙外的资源访问比较慢，建议先去官网下载。 自动下载和安装MNIST数据集，该数据会保存在当前目录MNIST_data下。 123import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) 创建一个交互式Session1sess = tf.InteractiveSession() 创建两个占位符shape是占位符分维度，第一个参数设置的None，它会自动计算维度。 x是输入特征，y_是期望 12x = tf.placeholder(&quot;float&quot;, shape=[None, 784])y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10]) 初始化权重及偏置1234567891011#权重初始化函数def weight_variable(shape): #输出服从截尾正态分布的随机值 initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#偏置初始化函数def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 创建卷积x 是一个4维张量，shape为[batch,height,width,channels]，卷积核移动步长为1。填充类型为SAME,可以不丢弃任何像素点 123def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=&quot;SAME&quot;) 创建池化采用最大池化，也就是取窗口中的最大值作为结果。x 是一个4维张量，shape为[batch,height,width,channels]；ksize表示pool窗口大小为2x2,也就是高2，宽2；strides，表示在height和width维度上的步长都为2； 123def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&quot;SAME&quot;) 前向传播和激活函数ReLU把x_image和权重进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling。h_pool1的输出即为第一层网络输出，shape为[batch,14,14,1] 1h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) 使用全连接层这层是拥有1024个神经元的全连接层，W的第1维size为7764，7*7是h_pool2输出的size，64是第2层输出神经元个数。 12W_fc1 = weight_variable([7*7*64, 1024])b_fc1 = bias_variable([1024]) Dropout层为了减少过拟合，在输出层前加入dropout 12keep_prob = tf.placeholder(&quot;float&quot;)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) softmax层输出时使用softmax将网络输出值转换成了概率 1y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 交叉墒预测值和真实值之间的交叉墒 1cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv)) 梯度下降使用ADAM优化器来做梯度下降。学习率为0.0001 1train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) 评估模型tf.argmax能给出某个tensor对象在某一维上数据最大值的索引。因为标签是由0,1组成了one-hot vector，返回的索引就是数值为1的位置 1correct_predict = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1)) 精确度计算计算正确预测项的比例，因为tf.equal返回的是布尔值，使用tf.cast把布尔值转换成浮点数，然后用tf.reduce_mean求平均值 1accuracy = tf.reduce_mean(tf.cast(correct_predict, &quot;float&quot;)) 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# -*- coding: utf-8 -*- import tensorflow as tf#导入input_data用于自动下载和安装MNIST数据集from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)#创建一个交互式Sessionsess = tf.InteractiveSession()#创建两个占位符，x为输入网络的图像，y_为输入网络的图像类别x = tf.placeholder(&quot;float&quot;, shape=[None, 784])y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10])#权重初始化函数def weight_variable(shape): #输出服从截尾正态分布的随机值 initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#偏置初始化函数def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)#创建卷积op#x 是一个4维张量，shape为[batch,height,width,channels]#卷积核移动步长为1。填充类型为SAME,可以不丢弃任何像素点def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=&quot;SAME&quot;)#创建池化op#采用最大池化，也就是取窗口中的最大值作为结果#x 是一个4维张量，shape为[batch,height,width,channels]#ksize表示pool窗口大小为2x2,也就是高2，宽2#strides，表示在height和width维度上的步长都为2def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&quot;SAME&quot;)#第1层，卷积层#初始化W为[5,5,1,32]的张量，表示卷积核大小为5*5，第一层网络的输入和输出神经元个数分别为1和32##########################11111111111111W_conv1 = weight_variable([5,5,1,32])#初始化b为[32],即输出大小b_conv1 = bias_variable([32])#把输入x(二维张量,shape为[batch, 784])变成4d的x_image，x_image的shape应该是[batch,28,28,1]#-1表示自动推测这个维度的sizex_image = tf.reshape(x, [-1,28,28,1])#把x_image和权重进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling#h_pool1的输出即为第一层网络输出，shape为[batch,14,14,1]h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)##########################22222222222h_pool1 = max_pool_2x2(h_conv1)#第2层，卷积层#卷积核大小依然是5*5，这层的输入和输出神经元个数为32和64##########################3333333W_conv2 = weight_variable([5,5,32,64])b_conv2 = weight_variable([64])#h_pool2即为第二层网络输出，shape为[batch,7,7,1]h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)##########################4444444h_pool2 = max_pool_2x2(h_conv2)#第3层, 全连接层#这层是拥有1024个神经元的全连接层#W的第1维size为7*7*64，7*7是h_pool2输出的size，64是第2层输出神经元个数##########################5555555W_fc1 = weight_variable([7*7*64, 1024])b_fc1 = bias_variable([1024])#计算前需要把第2层的输出reshape成[batch, 7*7*64]的张量h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)#Dropout层#为了减少过拟合，在输出层前加入dropoutkeep_prob = tf.placeholder(&quot;float&quot;)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)#输出层#最后，添加一个softmax层#可以理解为另一个全连接层，只不过输出时使用softmax将网络输出值转换成了概率W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)#预测值和真实值之间的交叉墒cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))#train op, 使用ADAM优化器来做梯度下降。学习率为0.0001train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#评估模型，tf.argmax能给出某个tensor对象在某一维上数据最大值的索引。#因为标签是由0,1组成了one-hot vector，返回的索引就是数值为1的位置correct_predict = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))#计算正确预测项的比例，因为tf.equal返回的是布尔值，#使用tf.cast把布尔值转换成浮点数，然后用tf.reduce_mean求平均值accuracy = tf.reduce_mean(tf.cast(correct_predict, &quot;float&quot;))#初始化变量sess.run(tf.global_variables_initializer())#开始训练模型，循环20000次，每次随机从训练集中抓取50幅图像for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: #每100次输出一次日志 train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_:batch[1], keep_prob:1.0&#125;) print (&quot;step %d, training accuracy %g&quot; % (i, train_accuracy)) train_step.run(feed_dict=&#123;x:batch[0], y_:batch[1], keep_prob:0.5&#125;)print (&quot;test accuracy %g&quot; % accuracy.eval(feed_dict=&#123;x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0&#125;)) 运行结果 训练所得结果如下图]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（三）tensorflow完整神经网络算法]]></title>
    <url>%2F2017%2F12%2F10%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89tensorflow%E5%AE%8C%E6%95%B4%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[完整神经网络经过前面的学习，对一个完整神经网路的实践已经迫不及待，下面的代码将展示关于神经网络的完整结构 定义输入占位符和反向传播占位符 初始化权值及偏置 定义前向传播过程 定义损失函数交叉熵 创建一个会话来运行 使用样本训练并更新参数 得到最终结果权值 要使用tensorflow首先需要导入tensorflow模块 1import tensorflow as tf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#coding=utf-8# 神经网络import tensorflow as tffrom numpy.random import RandomState#定义训练数据batch大小batch_size = 8#权值w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1),name="w1")w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1),name="w2")#定义输入占位符和反向传播占位符x = tf.placeholder(tf.float32, shape=(None,2), name="x-input")y_ = tf.placeholder(tf.float32, shape=(None,1), name="y-input")#定义前向传播过程a = tf.matmul(x,w1)y = tf.matmul(a,w2)#定义损失函数cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)#通过随机数产生模拟数据集rdm = RandomState(1)dataset_size = 128X = rdm.rand(dataset_size,2)Y = [[int(x1+x2&lt;1)] for (x1,x2) in X]#创建一个会话来运行with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) print(sess.run(w1)) print(sess.run(w2)) STEPS = 5000 for i in range(STEPS): start = (i * batch_size) % dataset_size end = min(start+batch_size,dataset_size) #使用样本训练并更新参数 sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;) if i % 1000 == 0: #计算一段时间内的交叉熵并输出 total_cross_entropy = sess.run(cross_entropy,feed_dict=&#123;x:X,y_:Y&#125;) print("After %d training step(s),cross_entropy on all data is %g" % (i,total_cross_entropy)) print(sess.run(w1)) print(sess.run(w2)) writer = tf.summary.FileWriter("./log",tf.get_default_graph()) writer.close() 输出结果: [[-0.81131822 1.48459876 0.06532937] [-2.4427042 0.0992484 0.59122431]] [[-0.81131822] [ 1.48459876] [ 0.06532937]] After 0 training step(s),cross_entropy on all data is 0.0674925 After 1000 training step(s),cross_entropy on all data is 0.0163385 After 2000 training step(s),cross_entropy on all data is 0.00907547 After 3000 training step(s),cross_entropy on all data is 0.00714436 After 4000 training step(s),cross_entropy on all data is 0.00578471 [[-1.96182752 2.58235407 1.68203771] [-3.46817183 1.06982315 2.11788988]] [[-1.82471502] [ 2.68546653] [ 1.41819501]] 通过tensorBoard生成结构图]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（二）tensorflow神经网络前向传播算法]]></title>
    <url>%2F2017%2F12%2F09%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[神经网络程序员学习的时候最重要的就是实践，以下代码是我自己手写，在自己搭建的jupyter tensorflow交互式环境下运行生成的。 要使用tensorflow首先需要导入tensorflow模块 1import tensorflow as tf constant123456789a = tf.constant([1.0,2.0],name="a",dtype=tf.float64)b = tf.constant([2.0,3.0],name="b",dtype=tf.float64)result = a + bprint(result)sess = tf.Session()x = sess.run(result)print(result)print(x)sess.close() 输出 Tensor(&quot;add_6:0&quot;, shape=(2,), dtype=float64) Tensor(&quot;add_6:0&quot;, shape=(2,), dtype=float64) [ 3. 5.] add_3:0表示result这个张量是计算节点“add”输出的第一个结果（编号从0开始） 会话 tensorflow的运算必须在会话（session）中计算 12345678910111213a = tf.constant([1.0,2.0],name="a",dtype=tf.float64)b = tf.constant([2.0,3.0],name="b",dtype=tf.float64)result = a + bprint(result)#交互式会话，使用这个函数会自动的将生成的会话注册为默认会话sess = tf.InteractiveSession()#sess = tf.Session()try: print(result.eval())except Exception as e: print(e) sess.close()sess.close() 输出 Tensor(&quot;add_10:0&quot;, shape=(2,), dtype=float64) [ 3. 5.] Variable123#随机生成矩阵，正态分布，标准差stddev=2weights = tf.Variable(tf.random_normal([2,3],stddev=2,dtype=tf.float64),name="weights")print(weights) 输出 &lt;tf.Variable &apos;weights_8:0&apos; shape=(2, 3) dtype=float64_ref&gt; 前向传播算法 神经网络的前向传播算法，简单的说就是矩阵的传播算法 123456789101112131415161718192021import tensorflow as tf#声明w1,w2两个变量，seed参数设定随机种子，这样保证每次运行的得到的结果一样w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1),name="w1")w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1),name="w2")# 假设输入的特征向量为一个常量，1*2的矩阵x = tf.constant([[0.7,0.9]],name="x")#前向传播算法a = tf.matmul(x,w1)y = tf.matmul(a,w2)#初始化变量with tf.Session() as sess: #writer = tf.train.SummaryWriter("./log",tf.get_default_graph()) writer = tf.summary.FileWriter("./log",tf.get_default_graph()) sess.run(tf.global_variables_initializer()) print(y.eval()) writer.close() 输出 [[ 3.95757794]] placeholder和前向传播算法 通过使用placeholder 实现神经网络的前向传播，placeholder是tensorflow中的占位符 12345678910111213141516import tensorflow as tfw1 = tf.Variable(tf.random_normal([2,3],stddev=1),name="w1")w2 = tf.Variable(tf.random_normal([3,1],stddev=1),name="w2")x = tf.placeholder(tf.float32,shape=(3,2),name="input")a = tf.matmul(x,w1)y = tf.matmul(a,w2)with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) result = sess.run(y,feed_dict=&#123;x:[[0.7,0.9],[0.1,0.4],[0.5,0.8]]&#125;) writer = tf.summary.FileWriter("./log",tf.get_default_graph()) writer.close() print(result) 输出 [[-4.83675146] [-1.65250468] [-4.0115037 ]] 损失函数、交叉熵12345678#定义损失函数刻画与测试和真实值的差距,交叉熵cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))#定义学习率learning_rate = 0.001#定义反向传播算法优化参数，AdamOptimizer优化算法的一种，目前有7种train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记（一）tensorflow+jupyter交互式环境搭建]]></title>
    <url>%2F2017%2F11%2F20%2F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89tensorflow%2Bjupyter%E4%BA%A4%E4%BA%92%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[tensorflow环境搭建经过对吴恩达机器学习课程的学习，理解了有关机器学习的基本理论，于是向着兴趣深度学习（deep learning）迈进。 基本步骤 安装Ubuntu 安装Python 安装Pip 安装Tensorflow 安装Jupyter 安装Nginx 绑定域名 使用交互式环境开发 安装Ubuntu64位Ubuntu作为人工智能学习最搭配的操作系统成为首选，关于Ubuntu安装这里不做详细阐述。 安装Python对tensorflow支持最好的python版本是3.5 ，所以这里使用python3。在Ubuntu系统终端中输入下面的命令安装： 1$ sudo apt-get install python3 安装Pippip是python的包管理器，使用pip安装python的各种模块非常方便，安装命令： 注意这里使用的是python3，因此pip版本也要安装pip3 1$ sudo apt-get install python3-pip 安装TensorflowTensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统，其命名来源于本身的运行原理。Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。 TensorFlow可被用于语音识别或图像识别等多项机器深度学习领域，对2011年开发的深度学习基础架构DistBelief进行了各方面的改进，它可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行。TensorFlow将完全开源，任何人都可以用。 有了上面的python3和pip3的安装，安装tensorflow就非常简单了，安装命令： 安装Tensorflow 1$ sudo pip3 install tensorflow 安装numpy numpy是一个用python实现的科学计算包。包括： 一个强大的N维数组对象Array； 比较成熟的（广播）函数库； 用于整合C/C++和Fortran代码的工具包； 实用的线性代数、傅里叶变换和随机数生成函数。 numpy和稀疏矩阵运算包scipy配合使用更加方便。 1$ sudo pip3 install numpy 安装SciPy SciPy是一款方便、易于使用、专为科学和工程设计的Python工具包.它包括统计,优化,整合,线性代数模块,傅里叶变换,信号和图像处理,常微分方程求解器等等. 1$ sudo pip3 install scipy 安装Pandas Python Data Analysis Library 或 pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。 1$ sudo pip3 install pandas 安装Matplotlib Matplotlib 是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。 1$ sudo pip3 install matplotlib 安装JupyterJupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。 Jupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。 数据挖掘领域中最热门的比赛 Kaggle 里的资料都是Jupyter格式。 1$ sudo pip3 install jupyter 启动命令 1$ jupyter notebook 然后在浏览器中输入刚刚启动界面显示的地址，比如我的地址是下面的： 1http://localhost:8888/?token=04cb8b5512b8680806499babab90c165dbca882adbe7b484 进入交互界面 尽情的使用吧 下面将讲述的是搭建基于jupyter的远程网络web服务器，这样不管走在哪里，使用手机还是电脑终端都可以使用自己的jupyter交互式环境开发学习。如果不需要搭建远程服务器，下面的可以略过 进行下面的步骤之前确认： 上面的环境都是在远程服务器上安装的（比如阿里云服务器） 购买域名（如果没有，使用ip地址访问） 安装NginxNginx (engine x) 是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。 1$ sudo apt-get install nginx 配置NGINX服务器 1$ sudo vi /etc/nginx/nginx.conf 在第13行# Basic Settings后面加上自己的配置 123456789101112131415161718192021222324252627282930313233upstream jupyter &#123; #jupyter所对应的ip和端口 ip_hash; #会话保存 server ip:端口; #如 192.23.12.12:8761 &#125; upstream tb &#123; #tensorflow可视化工具配置所对应的ip和端口 ip_hash; #session_sticky; # cookie=uid fallback=on mode=insert option=indirect;这是另一种配置，安装比较麻烦，这里不讲 server ip:端口; #如 192.23.12.12:8761 &#125; server &#123; server_name jupyter.域名; #如 jupyter.baidu.com #ssl证书文件位置(常见证书文件格式为：crt/pem) ssl_certificate /home/key/mycert.pem; #ssl证书key位置 ssl_certificate_key /home/key/mykey.key; #root /home/web/rootadmin_blog; location / &#123; proxy_pass http://jupyter; proxy_connect_timeout 600; proxy_read_timeout 600; #下面的不用改 但必须配上 不然交互式界面没有会话保存 ，不会跳转 proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # WebSocket support proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; &#125; &#125; 绑定域名设置域名解析，设置两条解析记录 1234# 这条记录让你的加不加www都一样（此处可以不配）Type:AHost:*IP:你的服务器ip 1234#这条很关键，可以让你在NGINX上任意映射端口和二级域名Type:AHost:@IP:你的服务器ip 使用交互式环境开发如果你的服务器上安装有防火墙ufw，需要打开80端口 1sudo ufw allow 80 在浏览器中使用自己的域名访问：jupyter.域名（如 jupyter.baidu.com）]]></content>
      <categories>
        <category>TensorFlow笔记</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（三）SVM支持向量机]]></title>
    <url>%2F2017%2F10%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[SVM支持向量机前言支持向量机SVM(Support Vector Machine)是常见的一种判别方法。在机器学习领域，是一个有监督的学习模型，通常用来进行模式识别、分类以及回归分析。深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法。 正文算法原理基本原理是寻找区分两类的超平面（hyper plane), 使边际(margin)最大。SVM的核心思想是尽最大努力使分开的两个类别有最大间隔，这样才使得分隔具有更高的可信度。而且对于未知的新样本才有很好的分类预测能力（在机器学习中叫泛化能力） 不太会打数学公式，SVM数学原理这里就不做说明了 线性不可分 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中 在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理 如图所示 核方法在线性SVM中转化为最优化问题时求解的公式计算都是以内积(dot product)的形式出现的，因为内积的算法复杂度非常大，所以我们利用核函数来取代计算非线性映射函数的内积 常用的核函数(kernel functions) 多项式核函数(polynomial kernel of degree h)： 高斯径向基核函数(Gaussian radial basis function kernel): S型核函数(Sigmoid function kernel): 如何选择使用哪个kernel？ 根据先验知识，比如图像分类，通常使用RBF，文字不使用RBF 尝试不同的kernel，根据结果准确度而定 SVM特点优点： 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting SVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。 一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化。 缺点： SVM对大规模样本难以实施 SVM解决多分类问题存在困难 Python实践1234567891011121314151617181920212223242526272829303132333435363738394041from __future__ import print_functionimport numpy as npimport pylab as plfrom sklearn import svm# 随机创建40个点X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]Y = [0]*20 +[1]*20# 使用线性核函数训练模型clf = svm.SVC(kernel=&apos;linear&apos;)clf.fit(X, Y)# 可视化过程w = clf.coef_[0]a = -w[0]/w[1]xx = np.linspace(-5, 5)yy = a*xx - (clf.intercept_[0])/w[1]b = clf.support_vectors_[0]yy_down = a*xx + (b[1] - a*b[0])b = clf.support_vectors_[-1]yy_up = a*xx + (b[1] - a*b[0])print(&quot;w: &quot;, w)print(&quot;a: &quot;, a)print(&quot;support_vectors_: &quot;, clf.support_vectors_)print(&quot;clf.coef_: &quot;, clf.coef_)# plot the line, the points, and the nearest vectors to the planepl.plot(xx, yy, &apos;k-&apos;)pl.plot(xx, yy_down, &apos;k--&apos;)pl.plot(xx, yy_up, &apos;k--&apos;)pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors=&apos;none&apos;)pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)pl.axis(&apos;tight&apos;)pl.show() 运行结果 结束到此，SVM算法介绍完毕]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二）K-NN算法（最邻近规则分类）]]></title>
    <url>%2F2017%2F10%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89K-NN%E7%AE%97%E6%B3%95%EF%BC%88%E6%9C%80%E9%82%BB%E8%BF%91%E8%A7%84%E5%88%99%E5%88%86%E7%B1%BB%EF%BC%89%2F</url>
    <content type="text"><![CDATA[kNN最邻近规则前言K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。 正文算法原理kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。 数据预处理 选择参数K 计算未知实例与所有已知实例之间的距离 选择K个已知实例 根据少数服从多数的投票法则，归类未知实例类别 算法特点 最近邻(kNN，k-NearestNeighbor)的优点： 简单，易于理解，易于实现，无需估计参数，无需训练 适合对稀有事件进行分类 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好。 最近邻(kNN，k-NearestNeighbor)的缺点： 需要大量空间存储已知实例 算法复杂度高，需要比较所有实例 当样本分布不均匀时，容易把未知实例分类为主导地位实例 改进 根据距离加权求和归类 Python实践 sklearn版本 sklearn中提供了很多机器学习库可以直接调用 1234567891011121314151617# -*- coding: utf-8 -*-from sklearn import neighborsfrom sklearn import datasets# sklearn中自带的knn模型knn = neighbors.KNeighborsClassifier()# sklearn中供knn学习的iris数据集iris = datasets.load_iris()print iris# 学习模型knn.fit(iris.data, iris.target)# 预测predictlabel = knn.predict([[0.4,0.3,0.5,0.2]])print predictedLabel 最后输出结果为0 自己实现 由于KNN算法相对于简单，所有这里也可以不掉用sklearn模块，自己手动实现算法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108# -*- coding: utf-8 -*-import csvimport randomimport mathimport operator# 定义函数 加载数据def loadDataset(filename, split, trainingSet=[], testSet=[]): with open(filename, &apos;rb&apos;) as csvfile: lines = csv.reader(csvfile) dataset = list(lines) for x in range(len(dataset)-1): for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random() &lt; split: #if random.randrange(len(trainingSet)) &lt; split: trainingSet.append(dataset[x]) else: testSet.append(dataset[x])# 计算实例时间的距离def euclideanDistance(instance1, instance2, length): distance = 0 for x in range(length): distance += pow((instance1[x]-instance2[x]), 2) return math.sqrt(distance)# 获得k个最近的邻居def getNeighbors(trainingSet, testInstance, k): distances = [] length = len(testInstance)-1 for x in range(len(trainingSet)): dist = euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key=operator.itemgetter(1)) neighbors = [] for x in range(k): neighbors.append(distances[x][0]) return neighbors# 获取结果def getResponse(neighbors): classVotes = &#123;&#125; for x in range(len(neighbors)): response = neighbors[x][-1] if response in classVotes: classVotes[response] += 1 else: classVotes[response] = 1 sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedVotes[0][0]# 获取准确率def getAccuracy(testSet, predictions): correct = 0 for x in range(len(testSet)): #print &apos;test&apos; # test = testSet[x][-1] # print test # print &apos;pre&apos; # pre = predictions[x] # print pre print (&apos;test: &apos; + repr(testSet[x][-1])) repr(testSet[x][-1]) print (&apos;pre: &apos; + repr(predictions[x])) # if testSet[z][-1] == predictions[z]: # correct += 1 return (correct/float(len(testSet)))*100.0# 主函数def main(): #prepare data &quot;&quot;&quot; :rtype: object &quot;&quot;&quot; trainingSet = [] testSet = [] split = 0.70 # 数据初始化 loadDataset(r&apos;./irisdata.txt&apos;, split, trainingSet, testSet) print &apos;Train set: &apos; + repr(len(trainingSet)) print &apos;Test set: &apos; + repr(len(testSet)) #generate predictions predictions = [] # 选取K值 k = 3 correct = [] for x in range(len(testSet)): # 计算未知实例与所有已知实例之间的距离，并取K个最近的实例 neighbors = getNeighbors(trainingSet, testSet[x], k) # 分类 result = getResponse(neighbors) predictions.append(result) #print (&apos;test: &apos; + repr(testSet)) print (&apos;predictions: &apos; + repr(predictions)) print (&apos;&gt;predicted=&apos; + repr(result) + &apos;, actual=&apos; + repr(testSet[x][-1])) if result == testSet[x][-1]: correct.append(x) # 计算准确率 accuracy = (len(correct)/float(len(testSet)))*100.0 print(&apos;Accuracy: &apos; + repr(accuracy) + &apos;%&apos;)if __name__ == &apos;__main__&apos;: main() 结束到此，KNN算法介绍完毕]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（一）决策树]]></title>
    <url>%2F2017%2F10%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[DecisionTree 决策树前言判定树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。决策树是机器学习中的一个重要算法。 决策树的优点： 直观，便于理解，小规模数据集有效 决策树的缺点： 处理连续变量不好 类别较多时，错误增加的比较快 可规模性一般 正文算法原理 ID3 (Information Gain) C4.5 （gain ratio): Quinlan Classification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone) 共同点：都是贪心算法，自上而下(Top-down approach) 区别：属性选择度量方法不同 这里介绍的ID3算法，ID3主要用的信息熵的分类方法。 ID3法DecisionTree信息熵信息是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。直到1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。信息熵这个词是C．E．香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。信息论之父克劳德·艾尔伍德·香农第一次用数学语言阐明了概率与信息冗余度的关系。 信息熵计算公式H(x) = E[I(xi)] = E[ log(2,1/p(xi)) ] = -∑p(xi)log(2,p(xi)) (i=1,2,..n) 其中，x表示随机变量，与之相对应的是所有可能输出的集合，定义为符号集,随机变量的输出用x表示。P(x)表示输出概率函数。变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大. 信息获取量信息获取量(Information Gain)：Gain(A) = Info(B) - Info_A(B) 通过A来作为节点分类获取了多少信息 算法演示，下面的表格为决策树需要处理的数据。 RID age income student credit_rating buy 1 youth high no fair no 2 youth high no excellent no 3 middle_aged high no fair yes 4 senior medium no fair yes 5 senior low yes fair yes 6 senior low yes excellent no 7 middle_aged low yes excellent yes 8 youth medium no fair no 9 youth low yes fair yes 10 senior medium yes fair yes 11 youth medium yes excellent yes 12 middle_aged medium no excellent yes 13 middle_aged high yes fair yes 14 senior medium no excellent no 首先我选择计算在没有任何特征值情况下的信息熵。这里令class_buys_computer=B，计算如下： Info(class_buys_computer) = Info(B) Info(B) = -5/14 log(5/14,2) - 9/14 log(9/14,2) 同理，令 age=A，则Info_A(B)表示用字段A得到字段B需要的信息量 Info_A(B) = 5/14 (-3/5 log(3/5,2)-2/5 log(2/5,2)) + 4/14 (-4/4 log(4/4,2)) + 5/14 (-3/5 log(3/5,2) - 2/5 log(2/5,2)) Gain(A) = Info(B) - Info_A(B) 用python计算下 123456789from math import logInfo_B = -5/14 * log(5/14,2) - 9/14 * log(9/14,2)print("Info(B) =",Info_B)Info_A_B = 5/14 * (-3/5 * log(3/5,2)-2/5 * log(2/5,2)) + 4/14 * (-4/4 * log(4/4,2)) + 5/14 * (-3/5 * log(3/5,2) - 2/5 * log(2/5,2))print("Info_A(B) =",Info_A_B)Gain_A = Info_B - Info_A_Bprint("Gain(A) =",Gain_A) Info(B) = 0.9402859586706309 Info_A(B) = 0.6935361388961918 Gain(A) = 0.2467498197744391 由此，我们计算出Gain(age) = 0.2467498197744391,同理求出其他特征的信息获取量。最终发现age的信息获取量最大，所以选age为根结点。用这样的方式继续选择下一个节点。 sklearn算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from sklearn.feature_extraction import DictVectorizerimport csvfrom sklearn import treefrom sklearn import preprocessingfrom sklearn.externals.six import StringIO# 读取数据allData = open(r'./data/decisiontree.csv', 'rt')reader = csv.reader(allData)headers = next(reader)print(headers)# 数据预处理featureList = []labelList = []for row in reader: labelList.append(row[len(row)-1]) rowDict = &#123;&#125; for i in range(1, len(row)-1): rowDict[headers[i]] = row[i] featureList.append(rowDict)print(featureList)vec = DictVectorizer()dummyX = vec.fit_transform(featureList) .toarray()print("dummyX: \n" + str(dummyX))print(vec.get_feature_names())print("labelList: " + str(labelList))# vectorize class labelslb = preprocessing.LabelBinarizer()dummyY = lb.fit_transform(labelList)print("dummyY: \n" + str(dummyY))# Using decision tree for classification# clf = tree.DecisionTreeClassifier()clf = tree.DecisionTreeClassifier(criterion='entropy')clf = clf.fit(dummyX, dummyY)print("clf: " + str(clf))# Visualize modelwith open("./save/decisiontreeGain.dot", 'w') as f: f = tree.export_graphviz(clf, feature_names=vec.get_feature_names(), out_file=f)oneRowX = dummyX[0, :]print("oneRowX: " + str(oneRowX))newRowX = oneRowXnewRowX[0] = 1newRowX[2] = 0print("newRowX: " + str(newRowX))# 预测predictedY = clf.predict(newRowX.reshape(-1, 10))print("predictedY: " + str(predictedY)) [&apos;RID&apos;, &apos;age&apos;, &apos;income&apos;, &apos;student&apos;, &apos;credit_rating&apos;, &apos;class_buys_computer&apos;] [{&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;youth&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;high&apos;}, {&apos;credit_rating&apos;: &apos;excellent&apos;, &apos;age&apos;: &apos;youth&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;high&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;middle_aged&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;high&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;senior&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;medium&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;senior&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;low&apos;}, {&apos;credit_rating&apos;: &apos;excellent&apos;, &apos;age&apos;: &apos;senior&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;low&apos;}, {&apos;credit_rating&apos;: &apos;excellent&apos;, &apos;age&apos;: &apos;middle_aged&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;low&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;youth&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;medium&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;youth&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;low&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;senior&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;medium&apos;}, {&apos;credit_rating&apos;: &apos;excellent&apos;, &apos;age&apos;: &apos;youth&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;medium&apos;}, {&apos;credit_rating&apos;: &apos;excellent&apos;, &apos;age&apos;: &apos;middle_aged&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;medium&apos;}, {&apos;credit_rating&apos;: &apos;fair&apos;, &apos;age&apos;: &apos;middle_aged&apos;, &apos;student&apos;: &apos;yes&apos;, &apos;income&apos;: &apos;high&apos;}, {&apos;credit_rating&apos;: &apos;excellent&apos;, &apos;age&apos;: &apos;senior&apos;, &apos;student&apos;: &apos;no&apos;, &apos;income&apos;: &apos;medium&apos;}] dummyX: [[ 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.] [ 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.] [ 1. 0. 0. 0. 1. 1. 0. 0. 1. 0.] [ 0. 1. 0. 0. 1. 0. 0. 1. 1. 0.] [ 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.] [ 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.] [ 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.] [ 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.] [ 0. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [ 0. 1. 0. 0. 1. 0. 0. 1. 0. 1.] [ 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.] [ 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.] [ 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.] [ 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]] [&apos;age=middle_aged&apos;, &apos;age=senior&apos;, &apos;age=youth&apos;, &apos;credit_rating=excellent&apos;, &apos;credit_rating=fair&apos;, &apos;income=high&apos;, &apos;income=low&apos;, &apos;income=medium&apos;, &apos;student=no&apos;, &apos;student=yes&apos;] labelList: [&apos;no&apos;, &apos;no&apos;, &apos;yes&apos;, &apos;yes&apos;, &apos;yes&apos;, &apos;no&apos;, &apos;yes&apos;, &apos;no&apos;, &apos;yes&apos;, &apos;yes&apos;, &apos;yes&apos;, &apos;yes&apos;, &apos;yes&apos;, &apos;no&apos;] dummyY: [[0] [0] [1] [1] [1] [0] [1] [0] [1] [1] [1] [1] [1] [0]] clf: DecisionTreeClassifier(class_weight=None, criterion=&apos;entropy&apos;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&apos;best&apos;) oneRowX: [ 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.] newRowX: [ 1. 0. 0. 0. 1. 1. 0. 0. 1. 0.] predictedY: [1] 生成的dot文件，里面包含的信息是决策树的文字描述，即描述了一幅图（决策树）。 安装GRAPHVIZ $ sudo apt-get install graphviz 123import oscomm="dot -Tpdf ./save/decisiontreeGain.dot -o ./save/decisiontree.pdf"result = os.system(comm) 0 运行结果 生成的决策树]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[品三国]]></title>
    <url>%2F2012%2F08%2F13%2F%E5%93%81%E4%B8%89%E5%9B%BD%2F</url>
    <content type="text"><![CDATA[三品三国： 一品三国，觉张飞之猛，关羽之傲，赵云之勇，天下无敌！ 二品三国，察孔明之智，公瑾之谋，仲达之奸，无之匹敌！ 再品三国，然国之奸雄，能谋善断者，独有孟德！三战徐州，官渡败袁，远征乌桓，平定关中，破黄巾，斩吕布，一生豪迈，诗情画意，短歌行，观沧海。临危制变，料敌设奇，赤壁虽败，难掩奇才！ 初偿三国，尢爱子龙，百万曹军，来去自如，长坂救嫂，七进七出，探囊取物，忠勇无敌。 次之，张飞关羽。力斩邓茂，鞭挞督邮，虎牢大战，大破徐州，当阳疑兵，吓退曹骑，计败张郃，此张飞也！ 阵斩颜良，单刀赴宴，温酒斩华雄，千里走单骑，此关羽也！ 后喜，诸葛孔明，躬耕南阳，隆中预言，天下三分，稳中求胜，百战不殆，草船借箭，空城御敌，出师表赋，更彰文采！ 公瑾周瑜，水师五万，赤壁败曹！ 司马仲达，大智若愚，韬光养晦，不鸣则以，一鸣惊人，斩孟达，取街亭，阻北伐！ 然，乱世三国，群雄角逐，各路英雄，皆亚曹操，东征西讨，用兵如神，礼贤下士，纵横天下，成王败寇，亘古不变！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
</search>
